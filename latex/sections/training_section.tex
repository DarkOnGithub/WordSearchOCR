\section{Entraînement et Analyse}

L'étape d'entraînement constitue la phase critique où le réseau de neurones ajuste ses poids synaptiques pour minimiser l'erreur de prédiction. Cette section détaille la méthodologie rigoureuse adoptée pour transformer une architecture théorique en un modèle performant, depuis la conception d'un jeu de données sur-mesure jusqu'à l'analyse fine de la convergence.

\subsection{Création des données et augmentation}
La fiabilité d'un système OCR est intrinsèquement liée à la représentativité de ses données d'apprentissage. Nous avons donc développé un \textbf{générateur de données synthétiques} capable de produire une infinité d'exemples annotés, simulant des conditions de dégradation réalistes.

Le pipeline de génération opère en deux temps :
\begin{enumerate}
    \item \textbf{Rendu Typographique :} Génération de caractères alphanumériques via une large bibliothèque de polices (Serif, Sans-serif, Manuscrit), assurant une diversité morphologique fondamentale.
    \item \textbf{Dégradation Stochastique :} Application d'une chaîne de transformations aléatoires pour modéliser les imperfections physiques et numériques.
\end{enumerate}

Les augmentations appliquées visent à immuniser le réseau contre les variations courantes :

\begin{itemize}
    \item \textbf{Altérations Physiques :} Simulation du vieillissement du papier (jaunissement, grain), de la texture et de la diffusion de l'encre (bavures).
    \item \textbf{Distorsions Géométriques :} Rotations aléatoires, corrections de perspective et déformations élastiques simulant des pages gondolées.
    \item \textbf{Artefacts de Numérisation :} Ajout de bruits de capteur (Gaussien, Speckle), flou de mise au point et pertes d'information locales (\textit{dropout}).
\end{itemize}

Ce processus a abouti à la création d'un corpus robuste de \textbf{164 424 images d'entraînement} et \textbf{29 016 images de validation}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{ressources/dataset_samples.png}
    \caption{Exemples de données synthétiques générées, illustrant la variété des polices et des dégradations.}
    \label{fig:dataset_samples}
\end{figure}

Une étape de \textbf{prétraitement} normalise les entrées : les pixels sont centrés et mis à l'échelle dans l'intervalle $[-1, 1]$, facilitant ainsi la dynamique d'optimisation du réseau.

\subsection{Configuration}
Pour garantir la reproductibilité de nos résultats, nous avons figé un ensemble d'hyperparamètres déterminés empiriquement pour offrir le meilleur compromis entre stabilité et vitesse de convergence.

\begin{table}[H]
    \centering
    \caption{Hyperparamètres de l'entraînement}
    \label{tab:hyperparams}
    \begin{tabular}{|l|l|p{6cm}|}
        \hline
        \textbf{Paramètre} & \textbf{Valeur} & \textbf{Justification} \\
        \hline
        Optimiseur & AdamW & Meilleure généralisation que Adam classique grâce au découplage de la décroissance de poids. \\
        \hline
        Taux d'apprentissage & $10^{-3}$ & Valeur standard permettant un démarrage rapide sans divergence. \\
        \hline
        Taille de lot (Batch) & 64 & Compromis équilibré entre l'estimation du gradient et l'occupation mémoire RAM. \\
        \hline
        Époques & 12 & Suffisant pour atteindre la convergence sans surapprentissage excessif. \\
        \hline
        Weight Decay & $10^{-4}$ & Régularisation L2 pour pénaliser les poids trop importants. \\
        \hline
    \end{tabular}
\end{table}

La session d'entraînement complète a nécessité \textbf{4 heures et 39 minutes}.

\subsection{Dynamique de Convergence}
L'analyse des courbes d'apprentissage est essentielle pour diagnostiquer le comportement du modèle.

\subsubsection{Analyse Globale (Inter-Epoch)}
Les graphiques ci-dessous (\ref{fig:epoch_performance}) montrent l'évolution des métriques moyennes à la fin de chaque époque.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{ressources/epoch_loss.png}
        \caption{Décroissance de la fonction de coût.}
        \label{fig:epoch_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{ressources/epoch_accuracy.png}
        \caption{Gain de précision.}
        \label{fig:epoch_accuracy}
    \end{subfigure}
    \caption{Évolution des performances globales sur 12 époques.}
    \label{fig:epoch_performance}
\end{figure}

\subsubsection{Stabilité Locale (Intra-Epoch)}
À l'échelle des micro-lots (batches), la variance observée (Fig. \ref{fig:batch_performance}) est naturelle. Elle résulte de la nature stochastique de l'optimisation : chaque lot ne contient qu'une approximation du gradient réel. Cette "agitation" est bénéfique car elle aide le modèle à s'échapper des minima locaux.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{ressources/batch_loss.png}
        \caption{Variance de la perte par lot.}
        \label{fig:batch_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{ressources/batch_accuracy.png}
        \caption{Précision instantanée.}
        \label{fig:batch_accuracy}
    \end{subfigure}
    \caption{Dynamique instantanée de l'apprentissage.}
    \label{fig:batch_performance}
\end{figure}

\subsection{Mécanismes de Contrôle et Régularisation}
Pour piloter finement la descente de gradient, nous avons implémenté des stratégies adaptatives.

\subsubsection{Ordonnancement du Taux d'Apprentissage (Scheduler)}
Une approche de type \textbf{Step Decay} a été retenue. L'idée est de réduire l'amplitude des mises à jour des poids à mesure que l'on s'approche de la solution optimale, pour "atterrir" en douceur au fond du minimum global.
Le taux d'apprentissage $\eta$ est multiplié par un facteur $\gamma = 0.1$ toutes les 7 époques.

\begin{algorithm}[H]
    \caption{Logique du StepLR Scheduler}
    \label{alg:steplr}
    \begin{algorithmic}[1]
        \Require $\eta_{init}$, $\gamma$ (facteur), $S$ (pas)
        \State $k \leftarrow \lfloor \text{epoch} / S \rfloor$
        \State $\eta_{new} \leftarrow \eta_{init} \times \gamma^{k}$
    \end{algorithmic}
\end{algorithm}

\subsubsection{Arrêt Précoce (Early Stopping)}
Pour éviter le surapprentissage (overfitting), où le modèle apprendrait "par cœur" les données d'entraînement sans généraliser, nous surveillons la performance sur le jeu de validation. Si celle-ci ne s'améliore pas durant \textbf{10 époques} consécutives, l'entraînement est stoppé.

\subsection{Bilan de Performance et Infrastructure}

\subsubsection{Métriques Finales}
À l'issue de l'entraînement, le modèle atteint des performances théoriques élevées :
\begin{itemize}
    \item \textbf{Précision Validation :} $96.60\%$
    \item \textbf{Précision Entraînement :} $97.72\%$
\end{itemize}
Le faible écart entre ces deux valeurs ($1.12\%$) témoigne d'une bonne généralisation sur les données synthétiques.

\subsubsection{Environnement de Calcul}
Les calculs ont été déportés sur un serveur \textbf{Intel Xeon Platinum 8488C}. Ce choix a permis de maintenir une latence d'inférence/rétropropagation constante de \textbf{500 ms/lot}, là où une machine grand public (i7-11800H) oscillait entre 700 et 1000 ms avec une forte variance thermique.

\subsection{Généralisation au Monde Réel}
Si les résultats quantitatifs sont excellents sur le jeu de données de test (généré synthétiquement), le passage aux données réelles a révélé des défis intéressants.

Nous avons noté une baisse de performance sur des lettres ambigues (ex: confusion O/Q ou I/l). Une comparaison directe avec une implémentation équivalente sous \textbf{PyTorch} a montré que notre implémentation \texttt{C}, bien que fonctionnelle, souffre d'un léger déficit de précision par rapport aux frameworks industriels.

Cette différence s'explique probablement par des optimisations numériques de bas niveau (gestion des arrondis, précision des accumulateurs dans les convolutions) que des librairies comme PyTorch gèrent avec une maturité extrême.
